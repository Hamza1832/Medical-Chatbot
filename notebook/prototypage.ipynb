{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c81c030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text length: 216167\n",
      "Cleaned text length: 109897\n",
      "Cleaned text saved to cleaned_book.txt\n",
      "Total chunks created: 35\n",
      "Sample chunk:\n",
      " a primer for patients and caregivers about braintumors  a primer for patients and caregiversabout braintumors 8550 W. Bryn Mawr Avenue, Suite   Chicago, IL 60631 CareLine:  - -ABTA (ÓÄ≤ÓÄ≤ ÓÄ≤) Email: info@abta.org Website: www.abta.org ABOUT THE  . Information contained in this publication was originally published in two volumes as Brain T umor Primer: A Comprehensive Introduction to Brain T umors, 9th Edition ; and Living with a Brain T umor: A Guide for Newly Diagnosed Patients and Their Families . ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£ Load PDF and extract text\n",
    "# =========================\n",
    "\n",
    "pdf_path = r\"C:\\Users\\hamza\\Desktop\\Medical-Chatbot\\data\\about-brain-tumors-a-primer-1.pdf\"   # use uploaded PDF\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "raw_text = \"\"\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \"\\n\"\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: Page {i} returned no text (image-based?)\")\n",
    "\n",
    "print(\"Raw text length:\", len(raw_text))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ Clean text\n",
    "# =========================\n",
    "\n",
    "def clean_text(text):\n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove common page numbers like:\n",
    "    # \"12\", \"Page 12\", \"12 AMERICAN BRAIN...\" etc.\n",
    "    text = re.sub(r'\\bPage\\s*\\d+\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d+\\s+AMERICAN BRAIN TUMOR ASSOCIATION\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d+\\s+www\\.abta\\.org\\b', '', text)\n",
    "\n",
    "    # Remove stray line-number-only lines\n",
    "    text = re.sub(r'\\n?\\b\\d{1,3}\\b\\n?', ' ', text)\n",
    "\n",
    "    # Remove repeated headers/footers from the uploaded PDF\n",
    "    patterns = [\n",
    "        r'AMERICAN BRAIN TUMOR ASSOCIATION.*?www\\.abta\\.org',\n",
    "        r'ABOUT BRAIN TUMORS\\s+.*?Caregivers'\n",
    "    ]\n",
    "\n",
    "    for p in patterns:\n",
    "        text = re.sub(p, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Final trim\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(\"Cleaned text length:\", len(cleaned_text))\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ Save cleaned text\n",
    "# =========================\n",
    "with open(\"cleaned_book.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "print(\"Cleaned text saved to cleaned_book.txt\")\n",
    "\n",
    "# =========================\n",
    "# Custom text splitter for RAG\n",
    "# =========================\n",
    "def split_text_custom(text, chunk_size=4000, overlap=800):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    length = len(text)\n",
    "\n",
    "    while start < length:\n",
    "        end = min(start + chunk_size, length)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# =========================\n",
    "# Load cleaned text\n",
    "# =========================\n",
    "with open(\"cleaned_book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_text = f.read()\n",
    "\n",
    "# =========================\n",
    "# Split into chunks\n",
    "# =========================\n",
    "chunks = split_text_custom(cleaned_text)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(\"Sample chunk:\\n\", chunks[0][:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c56e87ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.2\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb56cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m chunks = split_text_custom(cleaned_text, chunk_size=\u001b[32m4000\u001b[39m, overlap=\u001b[32m800\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal chunks created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample chunk:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m[:\u001b[32m500\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Custom text splitter for RAG\n",
    "# =========================\n",
    "def split_text_custom(text, chunk_size=4000, overlap=800):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of chunk_size characters with overlap.\n",
    "    Approx 1000 tokens ~ 4000 characters.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while start < text_length:\n",
    "        end = min(start + chunk_size, text_length)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  # move start by chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# =========================\n",
    "# Load cleaned text\n",
    "# =========================\n",
    "with open(\"cleaned_book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_text = f.read()\n",
    "\n",
    "# =========================\n",
    "# Split text into chunks\n",
    "# =========================\n",
    "chunks = split_text_custom(cleaned_text, chunk_size=4000, overlap=800)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(\"Sample chunk:\\n\", chunks[0][:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "863b5aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1/35\n",
      "\n",
      "--- Test similarity ---\n",
      "(10, 'ing  - -ABTA (2282). THE BRAIN TUMOR GUIDE For Newly Diagnosed Patients and Their Families  INTRODUCTION Causes and risk factors can be environmental , such as being exposed to poisonous substances in the home or at work; eating or not eating certain foods; or whether or not we exercise, smoke cigarettes or drink alcohol. They can also be genetic , such as being born with a gene mutation or susceptibility that one inherits from parents. These genetic mutations/susceptibilities may also accumulate over time, as one grows older. Unfortunately , no risk factor accounting for the majority of brain tumors has been identified, even though many environmental and genetic factors have been and are currently being studied. ENVIRONMENTAl f ACTORS Many studies have looked at a wide spectrum of environmental factors as possible causes of brain tumors including but not limited to: ‚Ä¢ Being exposed to air pollution, residential power lines, second hand smoke, agricultural chemicals and industrial formaldehyde ‚Ä¢ Working in synthetic rubber manufacturing or petroleum refining/production ‚Ä¢ Smoking cigarettes, smoking cigarettes while pregnant and consuming alcohol ‚Ä¢ Using common medications like birth control pills, sleeping pills, headache remedies, over-the-counter pain treatments and antihistamines ‚Ä¢ Having a history of head trauma, epilepsy , seizures or convulsions ‚Ä¢ Experiencing viruses and common infections ‚Ä¢ Consuming cured foods (nitrites) These exposures are difficult to accurately measure and can lead to inconsistencies across studies, making the results difficult to validate. Additional long-term research on these factors is needed before definite conclusions can be formed. Of the long list of factors studied, only exposure to ionizing radiation has been consistently associated with an increased risk for developing a brain tumor. (Ionizing radiation uses ‚Äúhigh-frequency‚Äù energy waves such as X-rays or gamma rays. However, radiation doses used today for medical and dental therapies are better focused than those used in medicine decades ago.) On the other hand, some studies have shown that a history of allergies as an adult, eating fruits and vegetables as a child and having a mother who ate fruits and vegetables during pregnancy and having chicken pox as a child puts one at a decreased risk of development of brain tumors. Of particular interest over the last decade has been the potential association between cell phone use and risk of developing a brain tumor. Multiple large studies have been performed in both the United States and Europe. Some have shown an association between cell phone use and brain tumor risk, while other studies show no association. In addition, studies have also investigated the difference in risk of a brain tumor between short-term and long-term (>  years) cell phone use with further conflicting results. In general, the conclusions from most of these studies are ( ) there is no consistent association between cell phone use and risk of developing a brain tumor (benign or malignant) and ( ) there is a very slight increased risk of a brain tumor associated with using a cell phone for   years or more. Further studies, in both the laboratory and in humans with longer follow up, are needed to fully understand this exposure and any potential relationship with brain tumor development. Additional research is needed before definite conclusions can be formed. ABOUT ClUSTERS Of BRAIN TUMORS Understandably , communities become concerned when several individuals within a neighborhood are diagnosed with brain tumors. Scientists studying these groups will want to learn whether these are metastatic brain tumors (those that CHAPTER  : CAUSES AND RISK fACTORS ‚Ä¢ Cyclin-dependent kinase inhibitors play a role in making sure that the cell goes through its growth cycle normally . ‚Ä¢ DNA repair genes make proteins that control accurate repair of damaged DNA. ‚Ä¢ Carcinogen metabolizing genes make proteins that break down toxic chemicals i', 0.8532605767250061)\n",
      "(11, 'aboratory and in humans with longer follow up, are needed to fully understand this exposure and any potential relationship with brain tumor development. Additional research is needed before definite conclusions can be formed. ABOUT ClUSTERS Of BRAIN TUMORS Understandably , communities become concerned when several individuals within a neighborhood are diagnosed with brain tumors. Scientists studying these groups will want to learn whether these are metastatic brain tumors (those that CHAPTER  : CAUSES AND RISK fACTORS ‚Ä¢ Cyclin-dependent kinase inhibitors play a role in making sure that the cell goes through its growth cycle normally . ‚Ä¢ DNA repair genes make proteins that control accurate repair of damaged DNA. ‚Ä¢ Carcinogen metabolizing genes make proteins that break down toxic chemicals in the body that could cause damage to one‚Äôs DNA, like the chemicals in cigarette smoke and/or alcohol. ‚Ä¢ Immune response genes make proteins that control how one‚Äôs immune system responds to viruses and infections. With the publication of the Human Genome and advances in genotyping technology , scientists can now identify over a million genetic variants found in the human body and ask the question: ‚ÄúAre any of these inherited genetic variants associated with risk of a brain tumor?‚Äù This type of study is called a genome-wide association (GWA) study . T wo recent GWA studies of glioma found some results in common, but they also found some differing results. The scientists involved in these studies believe the differences in their results may be due to the differences in the people who were part of their studies. This research shows that common genetic differences amongst the population can contribute to risk for developing a malignant brain tumor. Much more investigation is needed to fully understand the importance of these variations and how they may impact brain tumor risk. This type of GWA study has yet to be performed for benign brain tumors or pediatric brain tumors. ‚ÄúAcquired‚Äù or ‚Äúsomatic‚Äù means genetic changes that have accumulated over time. The Cancer Genome Atlas (TCGA) Project Studies of any specific gene are complicated by the fact that there are many potential genes in the human genome to consider. While these genes interact with one another, they may also interact with environmental factors as well. The Cancer Genome Atlas (TCGA) Project, funded by the National Cancer Institute (NCI) and National Human Genome Research Institute (NHGRI), has a goal of completely cataloging all of the somatic genetic changes in more than   different cancers, then making these data publically available in order to improve the ability to diagnose, treat and prevent cancer. TCGA started as a pilot project in 2006 prioritizing glioblastoma (GBM), ovarian and lung cancers as the first cancers to study . The first GBM paper published under this project showed three biological pathways involved with GBM. Since that publication, other scientists have described additional key genetic changes associated with malignant brain tumors. Some of these reports include important comparisons with low grade gliomas and other glioma subtypes. TCGA is now expanding its efforts to include other types and grades of gliomas. Chromosome Changes Another area of scientific study is the ability tumors have to lose or gain pieces of chromosomes . Each normal cell in any human body has   pairs of chromosomes. The most common chromosomal changes in brain tumors occur on chromosomes  ,  ,  ,  ,   and  . Changes on chromosomes   and   are most frequently found in oligodendrogliomas. Changes on chromosome   are most frequently found in meningiomas. Scientists are studying how this information can best be used for diagnostic or treatment purposes. CHAPTER  : CAUSES AND RISK fACTORS   www.abta.orgChapter  : Symptoms and Side Effects The symptoms of a brain tumor are different in each person. While it is not possible to know exactly what symptoms to expect, understanding what might occur', 0.8683999259233517)\n",
      "(19, 'nding what a seizure is and what to do if one should occur, can minimize fear and potential injury . For some people, a seizure may be the first clue that something unusual is happening in their brain. Seizures might be caused by a brain tumor or by the surgery to remove it. Seizures can also be totally unrelated to a brain tumor. For example, an injury to the head, a stroke, alcohol or drug withdrawal, and fever can all cause seizures. Or, the cause may be unknown. Most seizures can be controlled with medications called antiepileptic drugs (AEDs). Surgery or a ketogenic diet are also sometimes used to help treat ongoing seizures. This chapter provides information and resources to help people affected by seizures understand what they are experiencing and to learn how to live with this symptom. CHAPTER  : SEIZURES the person‚Äôs mouth, including your fingers. During a seizure, anything placed in the mouth will block the airway and cause breathing problems. Also, as the jaw often clenches during a seizure, your fingers could be bitten. Most seizures last several minutes. After the seizure ends, allow time for the per son to recover. They may be confused for a few moments. This is normal. Help re-orient them. Tell them who you are, where they are and what happened. Help them find a place to rest until they have recovered. Call for emergency assistance if: ‚Ä¢ The person is having difficulty breathing ‚Ä¢ The person injures himself ‚Ä¢ The seizure lasts more than   minutes ‚Ä¢ A second seizure immediately follows ‚Ä¢ The seizure occurs in water TYPES Of SEIZURES There are two primary types of seizures ‚Äì partial and generalized. The type you experience depends on which area of the brain has the abnormal electrical signals. Partial Seizures There are two types of partial seizures ‚Äì simple and complex. Simple Partial Seizures Simple partial seizures commonly cause jerking or twitching (if the frontal lobe is involved), tingling or numbness (if the parietal lobe is involved) or other sensations. These symptoms can begin in one part of the body and then spread to other areas. Chewing movements or lip smacking (if the anterior temporal lobe is involved), buzzing in the ears, flashes of lights, sweating, flushing and pupil dilation are other common symptoms. Psychic symptoms include a sense of d√©j√† vu, imaginary sights (if the occipital lobe is involved), smells (if the temporal lobe is involved), tastes or imaginary sounds. Simple partial seizures do not cause unconsciousness.Complex Partial Seizures Complex partial seizures cause some loss of consciousness and usually indicate temporal lobe involvement. Uncontrolled body movements might occur. The seizure may be preceded, accompanied by or followed by psychic symptoms. A state of confusion may continue after the seizure activity . In patients with low- grade gliomas, this is the most common type of seizure. Generalized Seizures These seizures may begin as partial seizures and abruptly change into generalized seizures. There are several different types of generalized seizures. Absence (Petit Mal) Seizures Absence seizures cause a brief delay in consciousness and may be accompanied by a feeling of limpness. The person having the seizure may miss a few words or stop speaking for a few seconds during a conversation. It may look like daydreaming. The beginning and end of the episode is usually sudden. This type of seizure most commonly begins in childhood and often stops by young adulthood. Atypical Absence Seizures Atypical absence seizures may cause more extensive changes in muscle tone or they may have a more gradual beginning and ending than typical absence seizures. Atonic Seizures (Drop Attacks) Atonic seizures are characterized by sudden limpness. Generally , all muscle tone and consciousness are lost. Myoclonic Seizures Myoclonic seizures cause single or multiple muscle twitches, jerks or spasms. Tonic-Clonic (Grand Mal) Seizures Tonic-clonic seizures are common in people with low-grade gliomas', 0.8714233564482117)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 0Ô∏è‚É£ Imports (keep your existing ones)\n",
    "# ------------------------------\n",
    "import psycopg\n",
    "from psycopg import Cursor\n",
    "import ollama\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------\n",
    "# 1Ô∏è‚É£ Variables\n",
    "# ------------------------------\n",
    "EMBED_MODEL = \"embeddinggemma\"  # Your Ollama embedding model\n",
    "db_connection_str = \"dbname=medical_rag user=postgres password=1803 host=localhost port=5432\"\n",
    "\n",
    "# If your chunks are already in memory\n",
    "# chunks = [...]  # list of strings from the previous splitting step\n",
    "\n",
    "# ------------------------------\n",
    "# 2Ô∏è‚É£ Helper functions (reuse yours)\n",
    "# ------------------------------\n",
    "\n",
    "def calculate_embeddings(corpus: str) -> list[float]:\n",
    "    response = ollama.embeddings(EMBED_MODEL, corpus)\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "def to_pgvector(vec: list[float]) -> str:\n",
    "    return \"[\" + \",\".join(str(v) for v in vec) + \"]\"\n",
    "\n",
    "def save_embedding(corpus: str, embedding: list[float], cursor: Cursor) -> None:\n",
    "    pg_vec = to_pgvector(embedding)\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO embeddings (corpus, embedding)\n",
    "        VALUES (%s, %s::vector)\n",
    "        \"\"\",\n",
    "        (corpus, pg_vec),\n",
    "    )\n",
    "\n",
    "def similar_corpus(input_corpus: str, k: int, cursor: Cursor):\n",
    "    embedding = calculate_embeddings(input_corpus)\n",
    "    pg_vec = to_pgvector(embedding)\n",
    "\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT id, corpus, embedding <=> %s::vector AS distance\n",
    "        FROM embeddings\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT %s\n",
    "        \"\"\",\n",
    "        (pg_vec, k),\n",
    "    )\n",
    "\n",
    "    return cursor.fetchall()\n",
    "\n",
    "# ------------------------------\n",
    "# 3Ô∏è‚É£ Store chunk embeddings in PostgreSQL\n",
    "# ------------------------------\n",
    "with psycopg.connect(db_connection_str) as conn:\n",
    "    conn.autocommit = True\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        # Drop old table if exists\n",
    "        cur.execute(\"DROP TABLE IF EXISTS embeddings\")\n",
    "\n",
    "        # Create extension pgvector\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "\n",
    "        # Create embeddings table\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS embeddings (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                corpus TEXT,\n",
    "                embedding VECTOR(768)\n",
    "            );\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Iterate through your chunks\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            emb = calculate_embeddings(chunk)\n",
    "            save_embedding(chunk, emb, cur)\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Processed chunk {i+1}/{len(chunks)}\")\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        # Optional: test similarity search\n",
    "        print(\"\\n--- Test similarity ---\")\n",
    "        test_results = similar_corpus(\"What causes inflammation?\", 3, cur)\n",
    "        for r in test_results:\n",
    "            print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== üéì MULTI-AGENT MEDICAL AI DEMO ==================\n",
      "\n",
      "\n",
      "üîç Starting analysis for: C:\\Users\\hamza\\Desktop\\Medical-Chatbot\\data\\Y10.jpg\n",
      "\n",
      "\n",
      "===================== üß† AI IMAGE ANALYSIS DEMO =====================\n",
      "\n",
      "‚îÄ‚îÄ AGENT 1: VISUAL FEATURES ‚îÄ‚îÄ\n",
      "   üëÅÔ∏è  Analyzing visual features...\n",
      "   üìÇ File: Y10.jpg\n",
      "\n",
      "‚îÄ‚îÄ AGENT 2: TEXTBOOK CONTEXT ‚îÄ‚îÄ\n",
      "   üî¨ Retrieving relevant textbook sections...\n",
      "\n",
      "‚îÄ‚îÄ AGENT 3: EDUCATIONAL ANALYSIS ‚îÄ‚îÄ\n",
      "   üìö Generating educational context...\n",
      "\n",
      "================== ‚úÖ ANALYSIS COMPLETE ==================\n",
      "\n",
      "üëÅÔ∏è VISUAL FEATURES\n",
      "----------------------------------------------------------------\n",
      "The image depicts a brain scan, likely an MRI or CT scan, with a visible tumor or lesion in the center. The tumor appears to be irregularly shaped, with a mix of bright and dark areas, indicating varying levels of intensity. The edges of the tumor are somewhat blurry, suggesting a possible boundary with surrounding tissue. There is a notable region of contrast between the tumor and the surrounding brain tissue, which could be indicative of a distinct difference in density or structure. The overall appearance of the tumor suggests it may be a complex or heterogeneous structure, possibly with multiple components or features. However, without further information or context, it is not possible to provide a definitive description of the tumor's characteristics or its potential impact on the surrounding brain tissue. \n",
      "\n",
      "üìö TEXTBOOK EDUCATIONAL CONTEXT\n",
      "----------------------------------------------------------------\n",
      "**Visual Features Analysis**\n",
      "\n",
      "The provided image appears to be a brain scan, likely an MRI or CT scan, showing various patterns of white matter hyperintensity (WMH) and cortical atrophy. These features can resemble the following imaging patterns:\n",
      "\n",
      "1. **Periventricular Leukomalacia (PVL)**: WMH in the periventricular region may indicate PVL, a condition commonly associated with premature birth or hypoxic-ischemic injury.\n",
      "2. **Cerebral Atrophy**: Generalized cortical atrophy can be seen in various neurodegenerative disorders, such as Alzheimer's disease, frontotemporal dementia, or cerebral amyloid angiopathy.\n",
      "3. **White Matter Lesions (WMLs)**: Small WMH may represent microinfarcts or small vessel disease, commonly associated with conditions like cerebral small vessel disease, lacunar stroke, or leukoencephalopathy.\n",
      "\n",
      "**Categories of Conditions**\n",
      "\n",
      "Textbooks often associate these patterns with the following categories of conditions:\n",
      "\n",
      "1. **Neurodegenerative Disorders**: Alzheimer's disease, frontotemporal dementia, cerebral amyloid angiopathy.\n",
      "2. **Small Vessel Disease**: Cerebral small vessel disease, lacunar stroke, leukoencephalopathy.\n",
      "3. **Premature Birth Complications**: Periventricular leukomalacia (PVL), hypoxic-ischemic encephalopathy.\n",
      "\n",
      "**Importance in Imaging**\n",
      "\n",
      "These patterns are considered important in imaging because they can:\n",
      "\n",
      "1. **Indicate Potential Underlying Condition**: WMH and cortical atrophy may indicate an underlying neurodegenerative or small vessel disease process.\n",
      "2. **Guide Further Evaluation**: These findings can guide the clinician to order additional tests, such as laboratory studies, electroencephalography (EEG), or other imaging modalities like functional MRI (fMRI) or positron emission tomography (PET).\n",
      "\n",
      "**Next Steps**\n",
      "\n",
      "Real clinicians would typically:\n",
      "\n",
      "1. **Order Laboratory Studies**: Complete blood counts, electrolyte panels, and liver function tests to assess overall health.\n",
      "2. **Conduct EEG or EMG**: To evaluate for potential underlying neurodegenerative disorders or small vessel disease.\n",
      "3. **Consider Additional Imaging Modalities**: Functional MRI (fMRI), positron emission tomography (PET), or single-photon emission computed tomography (SPECT) to assess cortical function, metabolism, or blood flow.\n",
      "\n",
      "**Disclaimer**\n",
      "\n",
      "Please note that this analysis is not a diagnosis and should not be used for clinical decision-making. A comprehensive medical evaluation by a qualified healthcare professional is necessary to establish an accurate diagnosis. This analysis is intended solely as an educational tool to familiarize the reader with potential imaging patterns and their associations with various conditions. \n",
      "\n",
      "üí° EDUCATIONAL INSIGHT\n",
      "----------------------------------------------------------------\n",
      "**Explanation of the Provided Text**\n",
      "\n",
      "This educational context demonstrates the application of artificial intelligence (AI) in medical image processing, specifically in analyzing brain scans. The text provides a detailed description of the visual features present in the image, including white matter hyperintensity (WMH) and cortical atrophy, which can be indicative of various neurodegenerative disorders or small vessel diseases.\n",
      "\n",
      "**1. AI Image Processing Demonstration**\n",
      "\n",
      "The provided text demonstrates the capabilities of AI in image processing by:\n",
      "* Identifying and describing visual features in the brain scan, such as WMH and cortical atrophy.\n",
      "* Associating these features with potential underlying conditions, including neurodegenerative disorders and small vessel diseases.\n",
      "* Highlighting the importance of these patterns in medical imaging and their potential impact on diagnosis and treatment.\n",
      "\n",
      "**2. Importance of Visual Patterns in Medical Imaging**\n",
      "\n",
      "Certain visual patterns in medical imaging, such as WMH and cortical atrophy, matter because they can:\n",
      "* Indicate underlying conditions, such as neurodegenerative disorders or small vessel diseases.\n",
      "* Guide further evaluation and testing, such as laboratory studies, EEG, or additional imaging modalities.\n",
      "* Inform treatment decisions and management strategies.\n",
      "\n",
      "**3. Clinician Evaluation of Suspicious Features**\n",
      "\n",
      "When evaluating suspicious features in medical imaging, clinicians typically:\n",
      "* Order laboratory studies to assess overall health and potential underlying conditions.\n",
      "* Conduct EEG or EMG to evaluate for potential underlying neurodegenerative disorders or small vessel disease.\n",
      "* Consider additional imaging modalities, such as functional MRI (fMRI), positron emission tomography (PET), or single-photon emission computed tomography (SPECT), to assess cortical function, metabolism, or blood flow.\n",
      "\n",
      "**4. Learning Opportunities for Students**\n",
      "\n",
      "This educational context provides students with the opportunity to learn about:\n",
      "* The application of AI in medical image processing and its potential benefits and limitations.\n",
      "* The importance of visual patterns in medical imaging and their association with various conditions.\n",
      "* The clinical evaluation process, including laboratory studies, EEG, and additional imaging modalities.\n",
      "* The importance of a comprehensive medical evaluation by a qualified healthcare professional in establishing an accurate diagnosis.\n",
      "\n",
      "**5. Reminder: Not a Diagnosis**\n",
      "\n",
      "It is essential to remember that this analysis is not a diagnosis and should not be used for clinical decision-making. A comprehensive medical evaluation by a qualified healthcare professional is necessary to establish an accurate diagnosis and develop an effective treatment plan. This educational context is intended solely as a tool to familiarize students with potential imaging patterns and their associations with various conditions. \n",
      "\n",
      "Sources used: 5\n",
      "================================================================\n",
      "\n",
      "\n",
      "üëã Exiting. Thanks for using the demo!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "import ollama\n",
    "from groq import Groq\n",
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------------\n",
    "# Variables\n",
    "# ------------------------------\n",
    "EMBED_MODEL = \"embeddinggemma\"\n",
    "LLM_MODEL = \"llama3\"\n",
    "VISION_MODEL = \"llama3.2-vision\"\n",
    "GROQ_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "db_connection_str = \"dbname=rag_chatbot user=postgres password=1803 host=localhost port=5432\"\n",
    "TOP_K = 5\n",
    "\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# ------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------\n",
    "\n",
    "def calculate_embeddings(corpus: str) -> list[float]:\n",
    "    response = ollama.embeddings(EMBED_MODEL, corpus)\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "def to_pgvector(vec: list[float]) -> str:\n",
    "    return \"[\" + \",\".join(str(v) for v in vec) + \"]\"\n",
    "\n",
    "def retrieve_chunks(query: str, k: int = TOP_K):\n",
    "    embedding = calculate_embeddings(query)\n",
    "    pg_vec = to_pgvector(embedding)\n",
    "\n",
    "    with psycopg.connect(db_connection_str) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                SELECT corpus, embedding <=> %s::vector AS distance\n",
    "                FROM embeddings\n",
    "                ORDER BY distance ASC\n",
    "                LIMIT %s\n",
    "                \"\"\",\n",
    "                (pg_vec, k)\n",
    "            )\n",
    "            results = cur.fetchall()\n",
    "    \n",
    "    return [r[0] for r in results]\n",
    "\n",
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# ------------------------------\n",
    "# AGENT 1: Vision Agent (SAFE + DESCRIPTIVE)\n",
    "# ------------------------------\n",
    "\n",
    "def vision_agent(image_path: str) -> dict:\n",
    "    print(\"   üëÅÔ∏è  Analyzing visual features...\")\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(image_path):\n",
    "            return {\"success\": False, \"error\": f\"Image not found: {image_path}\", \"description\": None}\n",
    "\n",
    "        print(f\"   üìÇ File: {os.path.basename(image_path)}\")\n",
    "\n",
    "        image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=VISION_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"\"\"\n",
    "You are NOT giving medical interpretation. ONLY describe visible visual features.\n",
    "\n",
    "Describe:\n",
    "- shapes (irregular, circular, diffuse, etc.)\n",
    "- brightness patterns\n",
    "- symmetry or asymmetry\n",
    "- edges (sharp, blurry, uneven)\n",
    "- regions of contrast\n",
    "- any notable structures\n",
    "\n",
    "Do NOT identify diseases. Only visual structure description.\n",
    "\"\"\",\n",
    "                    \"images\": [image_base64]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        description = response['message']['content']\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"description\": description,\n",
    "            \"image_path\": image_path\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e), \"description\": None}\n",
    "\n",
    "# ------------------------------\n",
    "# AGENT 2: Knowledge Agent (EDUCATIONAL, NON-DIAGNOSTIC)\n",
    "# ------------------------------\n",
    "\n",
    "def tumor_classification_agent(vision_description: str) -> dict:\n",
    "    print(\"   üî¨ Retrieving relevant textbook sections...\")\n",
    "\n",
    "    search_query = f\"brain imaging irregular mass appearance educational features {vision_description[:200]}\"\n",
    "    chunks = retrieve_chunks(search_query, TOP_K)\n",
    "\n",
    "    context = \"\\n\\n\".join(chunks) if chunks else \"No textbook data retrieved.\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You provide educational medical explanations. You NEVER diagnose.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "The following are visual features from an image:\n",
    "{vision_description}\n",
    "\n",
    "Textbook excerpts:\n",
    "{context}\n",
    "\n",
    "Provide:\n",
    "1. Educational explanation of what types of imaging features these resemble.\n",
    "2. What categories of conditions textbooks commonly associate with similar patterns.\n",
    "3. Why these patterns are considered important in imaging.\n",
    "4. What real clinicians would typically do next (tests, scans, evaluations).\n",
    "5. VERY CLEAR disclaimer: this is not diagnosis.\n",
    "\n",
    "Keep it detailed, structured, and educational.\n",
    "\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat(LLM_MODEL, messages=messages)\n",
    "    classification = response['message']['content']\n",
    "\n",
    "    return {\n",
    "        \"classification\": classification,\n",
    "        \"textbook_chunks_used\": len(chunks)\n",
    "    }\n",
    "\n",
    "# ------------------------------\n",
    "# AGENT 3: Educational Context Agent\n",
    "# ------------------------------\n",
    "\n",
    "def recommendation_agent(vision_description: str, classification: str) -> str:\n",
    "    print(\"   üìö Generating educational context...\")\n",
    "\n",
    "    try:\n",
    "        completion = groq_client.chat.completions.create(\n",
    "            model=GROQ_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You produce educational explanations about medical AI systems.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "VISUAL FEATURES:\n",
    "{vision_description}\n",
    "\n",
    "TEXTBOOK EDUCATIONAL CONTEXT:\n",
    "{classification}\n",
    "\n",
    "Explain:\n",
    "1. What this demonstrates about AI image processing.\n",
    "2. Why certain visual patterns matter in medical imaging.\n",
    "3. How clinicians actually evaluate suspicious features.\n",
    "4. What students should learn from this pipeline.\n",
    "5. Reminder: This is not diagnosis.\n",
    "\n",
    "Keep it clear and professional.\n",
    "\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=1200\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating educational info: {str(e)}\"\n",
    "\n",
    "# ------------------------------\n",
    "# ANALYSIS PIPELINE\n",
    "# ------------------------------\n",
    "\n",
    "def analyze_brain_tumor(image_path: str) -> dict:\n",
    "    print(\"\\n===================== üß† AI IMAGE ANALYSIS DEMO =====================\\n\")\n",
    "\n",
    "    # Agent 1\n",
    "    print(\"‚îÄ‚îÄ AGENT 1: VISUAL FEATURES ‚îÄ‚îÄ\")\n",
    "    vision_result = vision_agent(image_path)\n",
    "    if not vision_result[\"success\"]:\n",
    "        return {\"success\": False, \"error\": vision_result[\"error\"]}\n",
    "\n",
    "    # Agent 2\n",
    "    print(\"\\n‚îÄ‚îÄ AGENT 2: TEXTBOOK CONTEXT ‚îÄ‚îÄ\")\n",
    "    classification_result = tumor_classification_agent(vision_result[\"description\"])\n",
    "\n",
    "    # Agent 3\n",
    "    print(\"\\n‚îÄ‚îÄ AGENT 3: EDUCATIONAL ANALYSIS ‚îÄ‚îÄ\")\n",
    "    recommendations = recommendation_agent(\n",
    "        vision_result[\"description\"],\n",
    "        classification_result[\"classification\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"vision_analysis\": vision_result[\"description\"],\n",
    "        \"tumor_classification\": classification_result[\"classification\"],\n",
    "        \"recommendations\": recommendations,\n",
    "        \"textbook_sources\": classification_result[\"textbook_chunks_used\"],\n",
    "        \"image_path\": image_path\n",
    "    }\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN PROGRAM\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"\\n================== üéì MULTI-AGENT MEDICAL AI DEMO ==================\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"üí¨ Command: \").strip()\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            print(\"\\nüëã Exiting. Thanks for using the demo!\\n\")\n",
    "            break\n",
    "\n",
    "        if user_input.lower().startswith(\"analyze \"):\n",
    "            img = user_input.split(\" \", 1)[1].strip()\n",
    "\n",
    "            print(f\"\\nüîç Starting analysis for: {img}\\n\")\n",
    "            result = analyze_brain_tumor(img)\n",
    "\n",
    "            if not result[\"success\"]:\n",
    "                print(\"\\n‚ùå ERROR:\", result[\"error\"], \"\\n\")\n",
    "                continue\n",
    "\n",
    "            print(\"\\n================== ‚úÖ ANALYSIS COMPLETE ==================\\n\")\n",
    "\n",
    "            print(\"üëÅÔ∏è VISUAL FEATURES\")\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            print(result[\"vision_analysis\"], \"\\n\")\n",
    "\n",
    "            print(\"üìö TEXTBOOK EDUCATIONAL CONTEXT\")\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            print(result[\"tumor_classification\"], \"\\n\")\n",
    "\n",
    "            print(\"üí° EDUCATIONAL INSIGHT\")\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            print(result[\"recommendations\"], \"\\n\")\n",
    "\n",
    "            print(\"Sources used:\", result[\"textbook_sources\"])\n",
    "            print(\"================================================================\\n\")\n",
    "\n",
    "        else:\n",
    "            print(\"Unknown command. Use: analyze <path> or exit\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f6841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== MEDICAL MULTI-AGENT SYSTEM ==================\n",
      "\n",
      "\n",
      "üîç Starting analysis for: C:\\Users\\hamza\\Desktop\\Medical-Chatbot\\data\\Y10.jpg\n",
      "\n",
      "\n",
      "===================== AI IMAGE ANALYSIS PIPELINE =====================\n",
      "\n",
      "‚îÄ‚îÄ Agent 1: Vision ‚îÄ‚îÄ\n",
      "   üëÅÔ∏è  Agent 1 ‚Äî Analyzing visual features (non-diagnostic)...\n",
      "   üìÇ File: Y10.jpg\n",
      "Vision description obtained.\n",
      "\n",
      "‚îÄ‚îÄ Agent 2: Retrieval ‚îÄ‚îÄ\n",
      "   üîé Agent 2 ‚Äî Retrieving textbook passages...\n",
      "Retrieved 5 textbook passages.\n",
      "\n",
      "‚îÄ‚îÄ Agent 3: Enrichment ‚îÄ‚îÄ\n",
      "   üß† Agent 3 ‚Äî Enriching final output...\n",
      "‚ùå ERROR: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n",
      "Unknown command. Use: analyze <path> or exit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "import ollama\n",
    "from groq import Groq\n",
    "import os\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------------\n",
    "# Variables\n",
    "# ------------------------------\n",
    "EMBED_MODEL = \"embeddinggemma\"\n",
    "LLM_MODEL = \"llama3\"\n",
    "VISION_MODEL = \"llama3.2-vision\"\n",
    "GROQ_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GROQ_API_KEY = \"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\"  # <--- REPLACE WITH YOUR KEY\n",
    "\n",
    "db_connection_str = \"dbname=rag_chatbot user=postgres password=1803 host=localhost port=5432\"\n",
    "TOP_K = 5\n",
    "\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Ensure output folder exists\n",
    "if not os.path.exists(\"outputs\"):\n",
    "    os.makedirs(\"outputs\")\n",
    "\n",
    "# ------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------\n",
    "\n",
    "def calculate_embeddings(corpus: str) -> list[float]:\n",
    "    response = ollama.embeddings(EMBED_MODEL, corpus)\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "def to_pgvector(vec: list[float]) -> str:\n",
    "    return \"[\" + \",\".join(str(v) for v in vec) + \"]\"\n",
    "\n",
    "def retrieve_chunks(query: str, k: int = TOP_K):\n",
    "    embedding = calculate_embeddings(query)\n",
    "    pg_vec = to_pgvector(embedding)\n",
    "\n",
    "    with psycopg.connect(db_connection_str) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                SELECT corpus, embedding <=> %s::vector AS distance\n",
    "                FROM embeddings\n",
    "                ORDER BY distance ASC\n",
    "                LIMIT %s\n",
    "                \"\"\",\n",
    "                (pg_vec, k)\n",
    "            )\n",
    "            results = cur.fetchall()\n",
    "\n",
    "    return [r[0] for r in results]\n",
    "\n",
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# ------------------------------\n",
    "# AGENT 1: Vision Agent\n",
    "# ------------------------------\n",
    "\n",
    "def vision_agent(image_path: str) -> dict:\n",
    "    print(\"   üëÅÔ∏è  Agent 1 ‚Äî Analyzing visual features (non-diagnostic)...\")\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(image_path):\n",
    "            return {\"success\": False, \"error\": f\"Image not found: {image_path}\"}\n",
    "\n",
    "        print(f\"   üìÇ File: {os.path.basename(image_path)}\")\n",
    "        img = encode_image_to_base64(image_path)\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=VISION_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"\"\"\n",
    "Describe ONLY what is visually visible.\n",
    "NO diagnosis.\n",
    "\"\"\",\n",
    "                    \"images\": [img]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        desc = response[\"message\"][\"content\"].strip()\n",
    "        return {\"success\": True, \"description\": desc}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# ------------------------------\n",
    "# AGENT 2: Textbook Retrieval\n",
    "# ------------------------------\n",
    "\n",
    "def textbook_retrieval_agent(vision_description: str, top_k: int = TOP_K) -> dict:\n",
    "    print(\"   üîé Agent 2 ‚Äî Retrieving textbook passages...\")\n",
    "\n",
    "    try:\n",
    "        q = \"medical imaging features \" + vision_description[:800]\n",
    "        chunks = retrieve_chunks(q, top_k)\n",
    "\n",
    "        if not chunks:\n",
    "            return {\"success\": False, \"error\": \"No textbook passages retrieved.\", \"retrieved_chunks\": []}\n",
    "\n",
    "        return {\"success\": True, \"retrieved_chunks\": chunks, \"num_chunks\": len(chunks)}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e), \"retrieved_chunks\": []}\n",
    "\n",
    "# ------------------------------\n",
    "# AGENT 3: Enrichment (Groq)\n",
    "# ------------------------------\n",
    "\n",
    "def enrichment_agent(chunks: list[str], vision_description: str) -> dict:\n",
    "    print(\"   üß† Agent 3 ‚Äî Enriching final output...\")\n",
    "\n",
    "    try:\n",
    "        joined = \"\\n\\n-----\\n\\n\".join(chunks)\n",
    "        if len(joined) > 4000:\n",
    "            joined = joined[:4000] + \"\\n\\n[TRUNCATED]\"\n",
    "\n",
    "        system_msg = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Provide ONLY educational explanation. NO diagnosis. Must end with disclaimer.\"\n",
    "        }\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "Vision description:\n",
    "{vision_description}\n",
    "\n",
    "Textbook Passages:\n",
    "{joined}\n",
    "\n",
    "Task: produce an educational explanation, suggest typical clinical workflows (non-prescriptive), and end with a disclaimer.\n",
    "\"\"\"\n",
    "        }\n",
    "\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=GROQ_MODEL,\n",
    "            messages=[system_msg, user_msg],\n",
    "            max_tokens=1200,\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        final = response.choices[0].message.content.strip()\n",
    "        return {\"success\": True, \"final_text\": final}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# ------------------------------\n",
    "# PIPELINE\n",
    "# ------------------------------\n",
    "\n",
    "def analyze_pipeline(image_path: str) -> dict:\n",
    "    print(\"\\n===================== AI IMAGE ANALYSIS PIPELINE =====================\\n\")\n",
    "\n",
    "    # Agent 1\n",
    "    print(\"‚îÄ‚îÄ Agent 1: Vision ‚îÄ‚îÄ\")\n",
    "    v = vision_agent(image_path)\n",
    "    if not v[\"success\"]:\n",
    "        return {\"success\": False, \"error\": v[\"error\"]}\n",
    "    vision_desc = v[\"description\"]\n",
    "    print(\"Vision description obtained.\\n\")\n",
    "\n",
    "    # Agent 2\n",
    "    print(\"‚îÄ‚îÄ Agent 2: Retrieval ‚îÄ‚îÄ\")\n",
    "    t = textbook_retrieval_agent(vision_desc, TOP_K)\n",
    "    if not t[\"success\"]:\n",
    "        return {\"success\": False, \"error\": t[\"error\"]}\n",
    "    chunks = t[\"retrieved_chunks\"]\n",
    "    print(f\"Retrieved {t['num_chunks']} textbook passages.\\n\")\n",
    "\n",
    "    # Agent 3\n",
    "    print(\"‚îÄ‚îÄ Agent 3: Enrichment ‚îÄ‚îÄ\")\n",
    "    e = enrichment_agent(chunks, vision_desc)\n",
    "    if not e[\"success\"]:\n",
    "        return {\"success\": False, \"error\": e[\"error\"]}\n",
    "    print(\"Enrichment complete.\\n\")\n",
    "\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"vision_analysis\": vision_desc,\n",
    "        \"retrieved_chunks\": chunks,\n",
    "        \"final_synthesis\": e[\"final_text\"],\n",
    "        \"sources\": t[\"num_chunks\"]\n",
    "    }\n",
    "\n",
    "# ------------------------------\n",
    "# CLI\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"\\n================== MEDICAL MULTI-AGENT SYSTEM ==================\\n\")\n",
    "\n",
    "    while True:\n",
    "        cmd = input(\"üí¨ Command: \").strip()\n",
    "\n",
    "        if cmd.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"\\nüëã Exiting.\\n\")\n",
    "            break\n",
    "\n",
    "        if cmd.startswith(\"analyze \"):\n",
    "            image_path = cmd.split(\" \", 1)[1].strip()\n",
    "\n",
    "            print(f\"\\nüîç Starting analysis for: {image_path}\\n\")\n",
    "            result = analyze_pipeline(image_path)\n",
    "\n",
    "            if not result[\"success\"]:\n",
    "                print(\"‚ùå ERROR:\", result[\"error\"])\n",
    "                continue\n",
    "\n",
    "            # BUILD FILE NAME\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            out_path = f\"outputs/final_synthesis_{timestamp}.txt\"\n",
    "\n",
    "            # WRITE AGENT 3 OUTPUT TO FILE\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(result[\"final_synthesis\"])\n",
    "\n",
    "            print(\"\\n================== ANALYSIS COMPLETE ==================\\n\")\n",
    "            print(\"üëÅÔ∏è  AGENT 1 ‚Äî Visual Description (preview):\\n\", result[\"vision_analysis\"][:400], \"...\\n\")\n",
    "            print(\"üìö  AGENT 2 ‚Äî Passages Retrieved:\", result[\"sources\"])\n",
    "            print(\"üí°  AGENT 3 ‚Äî Full enriched output saved to:\")\n",
    "            print(\"   üìÑ\", out_path)\n",
    "            print(\"\\n=========================================================\\n\")\n",
    "\n",
    "            input(\"Press ENTER to continue...\")\n",
    "\n",
    "        else:\n",
    "            print(\"Unknown command. Use: analyze <path> or exit.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
